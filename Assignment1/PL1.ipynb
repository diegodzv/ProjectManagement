{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Project Managemente Assingment 1\n",
    "\n",
    "description: This document is a study on how different statistical and machine learning models can be used to predict data based on some parameters. \n",
    "\n",
    "authors: Diego Díaz Vidal, Marco Fernández Pérez, Lucía Patricia Gil Díaz\n",
    "\n",
    "date: 04/2024\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Effort Estimation\n",
    "\n",
    "In this part of the assingment we will study the relationship between different variables (size, resources, duration, etc) and the effort needed to carry out a project. For this purpose, we will obtain data from two datasets: `albretch.arff` and `china.arff`.\n",
    "Both datasets contain various observations of projects in which the effort is known (**supervised learning**). The goal is to train a machine learning model with some of that data, and then use the model to predict the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: albretch.arff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fort this task, we will use data from the \"albretch\" dataset. This dataset contains observations with two variables only: the size of the project and the effort needed.\n",
    "\n",
    "The following code imports the necessary libraries and downloads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import arff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "with open('./albretch.arff', 'r') as f:\n",
    "    data = arff.load(f)\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "size = np.array([row[0] for row in data['data']])\n",
    "effort = np.array([row[1] for row in data['data']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create and study two different estimation models and compare the results to see which one suits best to this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Linear Regression Model\n",
    "\n",
    "A linear regression model is a statistical method used for modeling the **relationship between a dependent variable** (also known as the target variable) **and one or more independent variables** (also known as the predictor variables). It asssumes that there is a linear relationship between the independent variables and the dependent variable.\n",
    "\n",
    "The basic idea behind the linear regression model is to find the best-fitting straight line that describes the relationship between the independent variables and the dependent variable. This line is called the **regression line**.\n",
    "\n",
    "The goal of the linear regression model is to **estimate the coefficients that minimize the sum of the squared differences between the observed values of the dependent variable and the values predicted by the regression line**.\n",
    "\n",
    "Once the coefficients are estimated, **the regression line can be used to make predictions** about dependent variables for new values of the independent variables.\n",
    "\n",
    "The following code creates a linear regression model that calculates those coefficients and then uses the regression line to calcule the **residuals**, as well as the **mean squared error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = np.polyfit(size, effort, 1)              # regression line\n",
    "effort_predicted = m * size + b                 # predict the effort\n",
    "\n",
    "residuals = effort - effort_predicted           # residuals\n",
    "mean_residual = np.mean(np.abs(residuals))      # mean residual\n",
    "\n",
    "mse = np.mean((effort - effort_predicted)**2)   # mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, due to the small number of observations, we decided to use all data for the training of the model.\n",
    "\n",
    "The results will be shown in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(f\"The Linear Regression is: Effort = {m:.2f}*Size + {b:.2f}\")\n",
    "print(f\"The Mean Squared Error is: {mse:.2f}\")\n",
    "print(f\"The Mean Residual is: {mean_residual:.2f}\")\n",
    "\n",
    "# Plot the data and the regression line\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(size, effort, color='blue', label='Data')\n",
    "plt.plot(size, effort_predicted, color='red', label='Linear Regression')\n",
    "plt.fill_between(size, effort_predicted - mean_residual, effort_predicted + mean_residual,\n",
    "                 color='green', alpha=0.2, label='Mean Residual Region')\n",
    "plt.title('Size vs Effort')\n",
    "plt.xlabel('Size')    \n",
    "plt.ylabel('Effort')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the mean residual is notably high, considering the value of the variables. This is probably because there is a weak relationship between the variables or high variability not explained by the model. This could be due to:\n",
    "\n",
    "1. **Nonlinearity in the data**: if this data does not follow a linear relationship; a linear model may not fit well, resulting in large residuals. We will later compare the results obtained with this linear model with the results obtained with a polynomial regression model.\n",
    "2. **Presence of outliers**: outliers can significantly affect the model and increase residuals, specially in a small dataset like this one.\n",
    "3. **Low number of observations**: the greater the number of observations the more robust the model is to outliers, and the more accurately it can identify the relationship between the variables (if exists)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Polynomial Regression Model\n",
    "\n",
    "After observing the results brought by the Linear Regression Model, we decided to further continue with the study of the relationship between the two variables by training another model: a polynomial regression model.\n",
    "\n",
    "While **the Linear Regression Model assumes a linear relationship between the variables**, **the Polynomial Model allows for a non-linear relationship**, which often offers a more accurate measure of the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = np.polyfit(size, effort, 2)      # polynomial regression\n",
    "\n",
    "# Function to calculate the predicted values\n",
    "def polyval(coefficients, x):\n",
    "    return coefficients[0] * x**2 + coefficients[1] * x + coefficients[2]\n",
    "\n",
    "effort_predicted = polyval(coefficients, size)  # predict the effort\n",
    "\n",
    "residuals = effort - effort_predicted           # residuals\n",
    "mean_residual = np.mean(np.abs(residuals))      # mean residual\n",
    "\n",
    "mse = np.mean((effort - effort_predicted)**2)   # mean squared error\n",
    "\n",
    "# Equation of the polynomial regression\n",
    "equation = f\"Effort = {coefficients[0]:.2f} * Size^2 + {coefficients[1]:.2f} * Size + {coefficients[2]:.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a polynomial model of degree 2 that studies the relationship between the size of the project and the effort needed. In this case, additionally to the mean residual, we also calculate the mean squared error to assess how well the polynomial regression captures the relationship.\n",
    "\n",
    "Now we will observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(\"Equation of the Polynomial Regression:\", equation)\n",
    "print(f\"The Mean Squared Error is: {mse:.2f}\")\n",
    "print(f\"The Mean Residual is: \", mean_residual)\n",
    "\n",
    "# Plot the data and the polynomial regression\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(size, effort, color='blue', label='Data')\n",
    "sorted_indices = np.argsort(size)\n",
    "plt.plot(size[sorted_indices], effort_predicted[sorted_indices], color='red', label='Polynomial Regression')\n",
    "plt.fill_between(size[sorted_indices], effort_predicted[sorted_indices] - mean_residual,\n",
    "                 effort_predicted[sorted_indices] + mean_residual, color='green',\n",
    "                 alpha=0.2, label='Mean Residual Region')\n",
    "plt.title('Size vs Effort')\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Effort')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Both models provide equations that express the effort as a function of size. To compare how well each model fits the actual data, we will use the following statistic:\n",
    "\n",
    "- Mean Residual: this value represents the average distance between the actual value and the predicted value by the curve.\n",
    "For the linear model, the mean residual is 143.14, while for the polynomial model is 110.45, which is lower. This means that the curve drawn by the polynomial model is, on average, closer to the actual values, resulting in a better performing model.\n",
    "- Mean Squared Error: while the linear model has a mse of 28697.99, the polynomial model's mse is 21549.97, also lower.\n",
    "\n",
    "These results demonstrate that the polynomial regression model fits the data better, making it a better candidate than the linear regression model when choosing a model to predict new outcomes. However, the results of both models are not as good as to be considered \"reliable\" for this case. This may be due to the low dependence between variables or the limited volume of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: china.arff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did before, in this part we will download a dataset containing data of various observations of projects.\n",
    "\n",
    "This dataset is much bigger than the previous one, so for convenience, we will convert it to csv format before starting to work with it.\n",
    "\n",
    "Additionally, since we have many more observations at our disposal, we can split the dataset into training and testing sets to perform a more thorough analysis of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Linear Regression Model\n",
    "\n",
    "Just like with the `albretch` dataset, here we will train a Linear Regression Model and evaluate its predicting capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Read the file\n",
    "data = pd.read_csv(\"chinaOriginal.csv\")\n",
    "\n",
    "# Divide the data into training and testing\n",
    "X = data.drop(columns=[\"Effort\"])\n",
    "y = data[\"Effort\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "linear_model = LinearRegression()               # create the model\n",
    "linear_model.fit(X_train, y_train)              # train the model using only training data\n",
    "y_pred_linear = linear_model.predict(X_test)    # predict the effort using testing data\n",
    "\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)  # mean squared error\n",
    "r2 = r2_score(y_test, y_pred_linear)                    # R^2 score\n",
    "\n",
    "residuals = y_test - y_pred_linear      # residuals\n",
    "average_residual = residuals.mean()     # average residual\n",
    "\n",
    "# Print Metrics\n",
    "print(\"Mean Squared Error:\", mse_linear)\n",
    "print(\"R2 Score:\", r2)\n",
    "print(\"Coefficients:\", linear_model.coef_)\n",
    "print(\"Intercept:\", linear_model.intercept_)\n",
    "print(\"Average Residual:\", average_residual)\n",
    "\n",
    "# Visualize the results of the predictions\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred_linear, color='blue', label='Actual Values vs Predictions')\n",
    "# Reference line (perfect prediction)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--', color='red', label='Perfect Predictions')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Actual Values vs Predictions (Linear Model)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the results for the residuals\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_pred_linear, residuals, color='blue', label='Residuals vs Predictions')\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Diagram (Linear Regression)')\n",
    "# Reference line (perfect residuals)\n",
    "plt.axhline(y=0, color='red', linestyle='--', label='Perfect Residuals')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Neural Network\n",
    "\n",
    "Multi-layer Perceptron Regressor (MLP) is a type of **artificial neural network that is used in regression tasks**. It consists of multiple layers of nodes (neurons) arranged in a feedforward manner, where each node in one layer is connected to every node in the subsequent layer.\n",
    "\n",
    "The **Training** of MLP Regressors **is carried out using optimization algorithms such as gradient descent to minimize a loss function, which measures the difference between the predicted and the actual values**.\n",
    "**Backpropagation is used to update the weights of the network iteratively during traininig**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Read the file\n",
    "data = pd.read_csv(\"chinaOriginal.csv\")\n",
    "\n",
    "# Divide the data into training and testing\n",
    "X = data.drop(columns=[\"Effort\"])\n",
    "y = data[\"Effort\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the Multi-layer Perceptron regressor Model\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n",
    "\n",
    "mlp_model.fit(X_train, y_train)         # train the model\n",
    "y_pred_mlp = mlp_model.predict(X_test)  # predict the effort\n",
    "\n",
    "# Metrics\n",
    "mse_mlp = mean_squared_error(y_test, y_pred_mlp)    # mean squared error\n",
    "r2_mlp = r2_score(y_test, y_pred_mlp)               # R^2 score\n",
    "\n",
    "residuals = y_test - y_pred_mlp         # residuals\n",
    "average_residual = residuals.mean()     # average residual\n",
    "\n",
    "# Print Metrics\n",
    "print(\"Mean Squared Error (RMSE):\", mse_mlp)\n",
    "print(\"R2 Score (MLPRegressor):\", r2_mlp)\n",
    "print(\"Average Residual:\", average_residual)\n",
    "\n",
    "# Visualize the results of the predictions\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred_mlp, color='blue', label='Actual Values vs Predictions')\n",
    "# Reference line (perfect prediction)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--', color='red', label='Perfect Predictions')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Actual Values vs Predictions (Neural Network)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the results for the residuals\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_pred_mlp, residuals, color='blue', label='Residuals vs Predictions')\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Diagram (MLP)')\n",
    "# Reference line (perfect residuals)\n",
    "plt.axhline(y=0, color='red', linestyle='--', label='Perfect Residuals')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Support Vector Regression\n",
    "\n",
    "Support Vector Regression (SVR) is a type of Support Vector Machine (SVM) algorithm that is used for **regression tasks**. Similar to classification using SVM, SVR also works by **finding the hyperplane that best fits the data**. However, **in SVR, the goal is to minimize the error between the predicted value and the actual value**, rather than simply classifying data points into different categories.\n",
    "\n",
    "The following code creates a SVR model and uses it to predict the effort based on the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Read the file\n",
    "data = pd.read_csv(\"chinaOriginal.csv\")\n",
    "\n",
    "# Divide the data into training and testing\n",
    "X = data.drop(columns=[\"Effort\"])\n",
    "y = data[\"Effort\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the SVR model\n",
    "svr_model = SVR(kernel='linear')\n",
    "\n",
    "svr_model.fit(X_train, y_train)         # train the model\n",
    "y_pred_svr = svr_model.predict(X_test)  # predict the effort\n",
    "\n",
    "# Metrics\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)    # mean squared error\n",
    "r2_svr = r2_score(y_test, y_pred_svr)               # R^2 score\n",
    "\n",
    "residuals_svr = y_test - y_pred_svr         # residuals\n",
    "average_residual_svr = residuals_svr.mean() # mean residuals\n",
    "\n",
    "# Print Metrics\n",
    "print(\"Mean Squared Error (SVR):\", mse_svr)\n",
    "print(\"R2 Score (SVR):\", r2_svr)\n",
    "print(\"Average Residual (SVR):\", average_residual_svr)\n",
    "\n",
    "# Visualize the results of the predictions\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred_svr, color='blue', label='Actual Values vs Predictions')\n",
    "# Reference line (perfect prediction)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--', color='red', label='Perfect Predictions')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Actual Values vs Predictions (Support Vector Regression)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the results for the residuals\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_pred_svr, residuals_svr, color='blue', label='Residuals vs Predictions')\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Diagram (SVR)')\n",
    "# Reference line (perfect residuals)\n",
    "plt.axhline(y=0, color='red', linestyle='--', label='Perfect Residuals')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclussions\n",
    "\n",
    "After studying the performance of multiple models (Linear, SVR and MLP) we have obtained the following results:\n",
    "\n",
    "|             |LINEAR   |MLP      |SVR      |\n",
    "|:-----------:|:-------:|:-------:|:-------:|\n",
    "|MSE          |1679987.5|647875.52|322104.17|\n",
    "|R2\t          |0.944    |0.978    |0.989    |\n",
    "|Mean Residual|-9.105   |38.43    |76.052   |\n",
    "\n",
    "According to these results:\n",
    "- The Support Vectorial Regression Model has the lowest MSE value, indicating better predictive quality.\n",
    "- The SVR also hast the best R2 coefficient.\n",
    "- Finally, we observe that the lineal model has a negative mean residual, which means that the model underestimated the value of the effort during the predictions, while the other two overestimated it.\n",
    "\n",
    "In conclussion, the SCR appears to be the most suitable for this dataset based on the MSE and R2 valuse. However, other factors such as training time and the interpretability of the model should be considered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
